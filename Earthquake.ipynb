{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dluLqPYHbBFK"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ec6e-gPfNyDX"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc0K4Fp5PsMi",
        "outputId": "a991345e-b54a-435f-fed1-68c97936bad3"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d88efd20b433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#LOADING DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_values.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'building_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_labels.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'building_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_values.csv'"
          ]
        }
      ],
      "source": [
        "#LOADING DATA \n",
        "train_values = pd.read_csv('train_values.csv', index_col='building_id')\n",
        "train_labels = pd.read_csv('train_labels.csv', index_col='building_id')\n",
        "\n",
        "train_values.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B69zB-RRP2Jf"
      },
      "outputs": [],
      "source": [
        "#EXPLORING DATA \n",
        "(train_labels.damage_grade\n",
        "             .value_counts()\n",
        "             .sort_index()\n",
        "             .plot.bar(title=\"Number of Buildings with Each Damage Grade\"))\n",
        "\n",
        "selected_features = ['foundation_type', \n",
        "                     'area_percentage', \n",
        "                     'height_percentage',\n",
        "                     'count_floors_pre_eq',\n",
        "                     'land_surface_condition',\n",
        "                     'has_superstructure_cement_mortar_stone']\n",
        "\n",
        "train_values_subset6 = train_values[selected_features]\n",
        "\n",
        "sns.pairplot(train_values_subset.join(train_labels), \n",
        "             hue='damage_grade')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fEH7Et6lVi0h"
      },
      "outputs": [],
      "source": [
        "#BUILDING THE MODEL \n",
        "# Random Forest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# for combining the preprocess with model training\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# for optimizing the hyperparameters of the pipeline\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z08aS-ZxVi7E"
      },
      "outputs": [],
      "source": [
        "# names the steps in your pipeline as a lowercase version of whatever the object name is\n",
        "#we can just take out the standardscaler\n",
        "#look at the documentation of the standardscaler - numeric, categorical .... \n",
        "#if subset model is similar, slightly lower we knows its working well \n",
        "\n",
        "train_values_full = pd.get_dummies(train_values)\n",
        "#Testing All Features\n",
        "pipe = make_pipeline(RandomForestClassifier(random_state=2018))\n",
        "pipe\n",
        "\n",
        "param_grid = {'randomforestclassifier__n_estimators': [50, 100],\n",
        "              'randomforestclassifier__min_samples_leaf': [1, 5]}\n",
        "# test a few different models using GridSearchCV              \n",
        "gs = GridSearchCV(pipe, param_grid, cv=5)\n",
        "\n",
        "gs.fit(train_values_full, train_labels.values.ravel())\n",
        "\n",
        "gs.best_params_\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "# sample f1 score\n",
        "in_sample_preds = gs.predict(train_values_full)\n",
        "\n",
        "# Compute the in-sample F1 score\n",
        "in_sample_f1 = f1_score(train_labels, in_sample_preds, average='micro')\n",
        "print(\"In-sample F1 score:\", in_sample_f1)\n",
        "\n",
        "# Create a cross-tab of the in-sample predictions and the true labels\n",
        "cross_tab = pd.crosstab(in_sample_preds, train_labels.values.ravel())\n",
        "print(cross_tab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sz5sk4DkrYZN"
      },
      "outputs": [],
      "source": [
        "#testing 6 features (given) \n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "train_values_subset6 = train_values[selected_features]\n",
        "train_values_subset6 = pd.get_dummies(train_values_subset6)\n",
        "# Create the pipeline\n",
        "pipe = make_pipeline(RandomForestClassifier(random_state=2018))\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {'randomforestclassifier__n_estimators': [50, 100],\n",
        "              'randomforestclassifier__min_samples_leaf': [1, 5]}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "gs = GridSearchCV(pipe, param_grid, cv=5)\n",
        "\n",
        "# Fit the model to the training data\n",
        "gs.fit(train_values_subset6, train_labels.values.ravel())\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters:\", gs.best_params_)\n",
        "\n",
        "# Get the in-sample predictions\n",
        "in_sample_preds = gs.predict(train_values_subset6)\n",
        "\n",
        "# Compute the in-sample F1 score\n",
        "in_sample_f1 = f1_score(train_labels, in_sample_preds, average='micro')\n",
        "print(\"In-sample F1 score:\", in_sample_f1)\n",
        "\n",
        "# Create a cross-tab of the in-sample predictions and the true labels\n",
        "cross_tab = pd.crosstab(in_sample_preds, train_labels.values.ravel())\n",
        "print(cross_tab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K1A1-9vgVit4"
      },
      "outputs": [],
      "source": [
        "#PREPROCESS THE DATA\n",
        "# train_values_full = pd.get_dummies(train_values) \n",
        "# train_values_subset22 = pd.get_dummies(train_values_subset22)\n",
        "# train_values_subset10 = pd.get_dummies(train_values_subset10)\n",
        "# train_values_subset6 = pd.get_dummies(train_values_subset6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "laIekNx6nE_Q"
      },
      "outputs": [],
      "source": [
        "# RIDGE  \n",
        "#should pull out a list of variables and coefficient sizes - for it to work everything has to be standardized \n",
        "#coefficients have meaning after scaling 0-1 or 1-100 \n",
        "#reduces coefficient sizes for unimportant variables and increases coefficient sizes for important variables - lasso will push to zero ridge will not\n",
        "#advantage allows it to maintain clusters of variables that are correlated to each other - useful for dataset with highly correlated variables\n",
        "# if we reduce and score is still high - model is solid! \n",
        "# use ridge for variable selection - pull out coefficient list - an argument inside the model after fitting\n",
        "#cross tabs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "X = train_values_full\n",
        "y = train_labels['damage_grade']\n",
        "\n",
        "# Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Standardize the training and test data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the ridge model\n",
        "model = Ridge(alpha=.5)\n",
        "\n",
        "# Fit the model to the standardized training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the list of coefficients\n",
        "coefficients = model.coef_\n",
        "\n",
        "# Print the coefficients\n",
        "print(coefficients)\n",
        "\n",
        "# Evaluate the model on the standardized test data\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "test_accuracy = model.score(X_test_scaled, y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NsIFTL0zDl9E"
      },
      "outputs": [],
      "source": [
        "def get_top_coefs(model, X, num_to_keep=0.10):\n",
        "    # Get the list of coefficients and the list of feature names\n",
        "    coefficients = model.coef_\n",
        "    feature_names = list(X.columns)\n",
        "\n",
        "    # Zip the coefficients and feature names together\n",
        "    coef_feature_tuples = list(zip(coefficients, feature_names))\n",
        "\n",
        "    # Sort the tuples by absolute value of the coefficient\n",
        "    sorted_tuples = sorted(coef_feature_tuples, key=lambda x: abs(x[0]))\n",
        "    \n",
        "    # Keep the top coefficients by absolute value\n",
        "    num_to_keep = int(len(sorted_tuples) * num_to_keep)\n",
        "    top_coefs = sorted_tuples[-num_to_keep:]\n",
        "    \n",
        "    return top_coefs\n",
        "\n",
        "# Get the top 15% of coefficients\n",
        "top_coefs = get_top_coefs(model, X, num_to_keep=0.10)\n",
        "\n",
        "# Print the top coefficients\n",
        "print(top_coefs)\n",
        "top10_coefs = ['has_secondary_use', \n",
        "                     'foundation_type_r', \n",
        "                     'foundation_type_w',\n",
        "                     'has_superstructure_cement_mortar_brick',\n",
        "                     'has_secondary_use_use_police',\n",
        "                     'plan_configuration_q',\n",
        "                     'has_superstructure_mud_mortar_stone', \n",
        "                     'has_secondary_use_school', \n",
        "                     'has_superstructure_stone_flag',\n",
        "                     'has_superstructure_rc_engineered' \n",
        "               ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tat9Ef98JWSt"
      },
      "outputs": [],
      "source": [
        "#testing top 10 Coefs from Ridge Regression  \n",
        "train_values_subset10 = train_values_full[top10_coefs]\n",
        "train_values_subset10 = pd.get_dummies(train_values_subset10)\n",
        "\n",
        "pipe = make_pipeline(RandomForestClassifier(random_state=2018))\n",
        "pipe\n",
        "\n",
        "param_grid = {'randomforestclassifier__n_estimators': [50, 100],\n",
        "              'randomforestclassifier__min_samples_leaf': [1, 5]}\n",
        "# test a few different models using GridSearchCV              \n",
        "gs = GridSearchCV(pipe, param_grid, cv=5)\n",
        "\n",
        "gs.fit(train_values_subset10, train_labels.values.ravel())\n",
        "\n",
        "gs.best_params_\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "# sample f1 score\n",
        "in_sample_preds = gs.predict(train_values_subset10)\n",
        "\n",
        "# Compute the in-sample F1 score\n",
        "in_sample_f1 = f1_score(train_labels, in_sample_preds, average='micro')\n",
        "print(\"In-sample F1 score:\", in_sample_f1)\n",
        "\n",
        "# Create a cross-tab of the in-sample predictions and the true labels\n",
        "cross_tab = pd.crosstab(in_sample_preds, train_labels.values.ravel())\n",
        "print(cross_tab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OzLrOtbM05g1"
      },
      "outputs": [],
      "source": [
        "# BACKWARDS SELECTION \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "X = train_values_full\n",
        "y = train_labels['damage_grade']\n",
        "\n",
        "# Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Initialize the list of features to include in the model\n",
        "features = list(X_train.columns)\n",
        "\n",
        "# Initialize the list of scores for each iteration\n",
        "scores = []\n",
        "\n",
        "# Iterate over a range of number of features to remove\n",
        "for num_to_remove in range(len(features)):\n",
        "    # Fit the model with the current set of features\n",
        "    model.fit(X_train[features], y_train)\n",
        "    \n",
        "    # Get the score for the current set of features\n",
        "    score = model.score(X_test[features], y_test)\n",
        "    \n",
        "    # Append the score to the list of scores\n",
        "    scores.append(score)\n",
        "\n",
        "    # If this is the best score so far, store the features\n",
        "    if score == max(scores):\n",
        "        best_features = features.copy()\n",
        "\n",
        "    # Print the best set of features and the corresponding score\n",
        "print(\"Best set of features:\", best_features)\n",
        "print(\"Best score:\", max(scores))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vh-Cmiqr771O"
      },
      "outputs": [],
      "source": [
        "#testing backwards selection (22 Coef)\n",
        "train_values_subset22 = train_values_full[best_features]\n",
        "train_values_subset22 = pd.get_dummies(train_values_subset22)\n",
        "\n",
        "pipe = make_pipeline(RandomForestClassifier(random_state=2018))\n",
        "pipe\n",
        "\n",
        "param_grid = {'randomforestclassifier__n_estimators': [50, 100],\n",
        "              'randomforestclassifier__min_samples_leaf': [1, 5]}\n",
        "# test a few different models using GridSearchCV              \n",
        "gs = GridSearchCV(pipe, param_grid, cv=5)\n",
        "\n",
        "gs.fit(train_values_subset22, train_labels.values.ravel())\n",
        "\n",
        "gs.best_params_\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "# sample f1 score\n",
        "in_sample_preds = gs.predict(train_values_subset22)\n",
        "\n",
        "# Compute the in-sample F1 score\n",
        "in_sample_f1 = f1_score(train_labels, in_sample_preds, average='micro')\n",
        "print(\"In-sample F1 score:\", in_sample_f1)\n",
        "\n",
        "# Create a cross-tab of the in-sample predictions and the true labels\n",
        "cross_tab = pd.crosstab(in_sample_preds, train_labels.values.ravel())\n",
        "print(cross_tab)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}